---
title: "PS3 506 Yeomans"
author: "Sydney Yeomans"
format:
  html:
    embed-resources: true
editor: visual
---

## Problem 1

For the “nice tables”, use a function such as kable from knitr, or the stargazer package (or find another approach) to generate HTML/LaTeX tables for inclusion. The results should be clearly labeled, rounded appropriately, and easily readable.

```{r}
install.packages("stargazer")
library(stargazer)
```

a. Merge the two files to create a single data.frame. Keep only records which matched. Print out the dimensions of the merged data.frame.

```{r}
#install.packages("haven")
library(haven)
aux_df <- read_xpt("AUX_I.xpt")
dim(aux_df)
demo_df <- read_xpt("DEMO_I.xpt")
dim(demo_df)
#they have to match based on the variable SEQN which is Respondent sequence number
merged_df <- merge(aux_df, demo_df, by = "SEQN")
dim(merged_df)
```

Since this is a SAS file I looked up how to read it into R and found this source <https://www.rdocumentation.org/packages/haven/versions/2.0.0/topics/read_xpt>. I then looked up if this is in base R and it is not so I found what package it comes from. I looked up how to merge data set and used this site to help me merge the two <https://www.datacamp.com/doc/r/merging>. 


b. Clean up each - ensure all missing values are actually NA (rather than 999 or something), and if it’s categorical, convert it to factor with informative levels. Variables: Gender, Citizenship status, Number of children 5 years or younger in the household, and Annual household income - There’s also an issue with the ordering of the categories here; take a look, identify the issue, and implement a solution.

```{r}

```


c. The Tympanometric width measure is looks approximately like a Poisson distribution. Fit four Poisson regression models predicting a respondent’s Tympanometric width in each ear. Each model is defined below, for a specific ear and a specific set of covariates.

```{r}

```



d.

```{r}

```



## Problem 2

Use the “sakila” database discussed in class. For each of the following questions, solve them in two ways: First, use SQL query or queries to extract the appropriate table(s), then use regular R operations on those data frames to answer the question. Second, use a single SQL query to answer the question. Compare each approach using microbenchmark.

```{r}
#read in data and add the needed libraries:
#install.packages("RSQLite")
library(RSQLite)
#install.packages("DBI")
library(DBI)
sakila <-  dbConnect(RSQLite::SQLite(), "sakila_master.db")

```

SQL version:

a. For each store, how many customers does that store have, and what percentage of customers of that store are active in the system?

```{r}
#dbListTables(sakila)

gg <- function(query) {
  dbGetQuery(sakila, query)
}


```

b. Generate a table identifying the names and country of each staff member.

```{r}

```

c. Identify the name(s) of the film(s) which was/were rented for the highest dollar value. (Assume all costs are in USD regardless of country.) (Hint: You can merge a table more than once.)

```{r}

```


Base R and data frame version:

a.

```{r}

```

b.

```{r}

```

c.

```{r}

```


Single SQL querey:

a.

```{r}

```


b.

```{r}

```


c.

```{r}

```



## Problem 3

Download the ``Australia - 500 Records'' data. Use it to answer the following questions.

```{r}
#| echo: true
aus_df <- read.csv("au-500.csv")
```

a. What percentage of the websites are .com’s (as opposed to .net, .com.au, etc)?

```{r}
#| echo: true
#grep("\\.com\\.au", aus_df$web)
table(grepl("\\.com\\.au", aus_df$web))
```
Zero percent of the websites in the data set end in .com. All of the websites end in .com.au, which can be seen just taking a look at the data set but this gives us concrete proof that this is the case and our eyes aren't tricking us.

b. What is the most common domain name amongst the email addresses?

```{r}
#| echo: true
domains <- sapply(strsplit(aus_df$email, "@"), "[[", 2)
table(domains)
```

The most common domain name in the email addresses is Hotmail with 114 occurrences, followed by Gmail with 102 and Yahoo with 84. The code I used for this was taken from <https://stackoverflow.com/questions/31235165/sapply-with-strsplit-in-r> where they are keeping the part after a certain symbol.

c. What proportion of company names contain a non-alphabetic character, excluding commas and whitespace. (E.g. “Jane Doe, LLC” would not contain an eligible non-alphabetic character; “Plumber 247” would.) What about if you also exclude ampersands (“&”)?

```{r}
#| echo: true
clean_company_name <- gsub(" ", "", aus_df$company_name, fixed = TRUE)
no_comma <- gsub(",", "", clean_company_name, fixed = TRUE)
table(grepl("[^A-Za-z]", no_comma))
(45 / 500) * 100
```

Excluding commas and whitespace, we see that $9%$ of the company names have a non-alphabetic character.

```{r}
#| echo: true
no_ampersand <- gsub("&", "", no_comma, fixed = TRUE)
table(grepl("[^A-Za-z]", no_ampersand))
(4 / 500) * 100
```

Adding in the character ``&'' we then see the proportion drops down to $0.8%$, which is really small. 


d. Make all phone numbers written like cell phones. Show it works by printing the first 10 phone numbers of each column.

```{r}
#| echo: true
#my idea is to strip the landline one of the - then place them 
#such that they match the pattern of the cell phoen
landline <- gsub("-", "", aus_df$phone1, fixed = TRUE)
aus_df$phone1 <- gsub("(\\d{4})(\\d{3})(\\d{3})$","\\1-\\2-\\3", landline)
head(aus_df$phone1, 10)
head(aus_df$phone2, 10)

```

The site I used to come up with how to place the hypens in the needed spots was here: <https://stackoverflow.com/questions/41655171/what-is-an-elegant-way-to-add-dashes-in-number-strings>. Now the landline numbers match that of the cellphone. 

e. Produce a histogram of the log of the apartment numbers for all addresses. (You may assume any number at the end of the an address is an apartment number.)

```{r}
#| echo: true
#get the number after the '#' in the addresses
aus_df <- aus_df[grepl("#", aus_df$address), ]
apt_split <- apt_split <- strsplit(aus_df$address, "#")
apt_num <- sapply(apt_split,  "[[", 2)
apt_num <- as.numeric(apt_num)
head(apt_num)
hist(log(apt_num))
```
The numbers seen from head are the first apartment numbers that show up in the CSV. I used the same apply function as part b, which comes from that same site from earlier. 

f. Do you think the apartment numbers would pass as real data? Based on Benford's law

Benford's law is an observation that in many real-life sets of numerical data, the leading digit is likely to be small, meaning our histogram should show lots of values that fall into the smaller number categories compared to the larger values. From our histogram in the last part of this problem, we can see this is not the case. Our histogram of apartment numbers we see that log 0 to 1 has one of the lowest counts with a frequency of 5 then it goes up and down as we go further up on the x-axis, with no set pattern. Therefore, the apartment numbers do not pass as real data, which matches the fact that this data was made up.








