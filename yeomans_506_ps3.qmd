---
title: "PS3 506 Yeomans"
author: "Sydney Yeomans"
format:
  html:
    embed-resources: true
editor: visual
---

## Problem 1

For the “nice tables”, use a function such as kable from knitr, or the stargazer package (or find another approach) to generate HTML/LaTeX tables for inclusion. The results should be clearly labeled, rounded appropriately, and easily readable.

a. Merge the two files to create a single data.frame. Keep only records which matched. Print out the dimensions of the merged data.frame.

```{r}
#| echo: true
#install.packages("haven")
library(haven)
aux_df <- read_xpt("AUX_I.xpt")
dim(aux_df)
demo_df <- read_xpt("DEMO_I.xpt")
dim(demo_df)
#they have to match based on the variable SEQN - Respondent sequence number
merged_df <- merge(aux_df, demo_df, by = "SEQN")
dim(merged_df)
```

Since this is a SAS file I looked up how to read it into R and found this source <https://www.rdocumentation.org/packages/haven/versions/2.0.0/topics/read_xpt>. I then looked up if this is in base R and it is not so I found what package it comes from. I looked up how to merge data set and used this site to help me merge the two <https://www.datacamp.com/doc/r/merging>. 


b. Clean up each - ensure all missing values are actually NA (rather than 999 or something), and if it’s categorical, convert it to factor with informative levels. Variables: Gender, Citizenship status, Number of children 5 years or younger in the household, and Annual household income - There’s also an issue with the ordering of the categories here; take a look, identify the issue, and implement a solution.

```{r}
#| echo: true
#gender = RIAGENDR
#citizenship = DMDCITZN
#number of children aged 5 years or younger in the household = DMDHHSZA 
#annual household income = INDHHIN2

#start with gender: 1 = male, 2 = female, if any other number set as NA
merged_df$RIAGENDR[merged_df$RIAGENDR != 1 & merged_df$RIAGENDR != 2] <- NA
merged_df$RIAGENDR <- factor(merged_df$RIAGENDR, levels = c(1, 2), 
                             labels = c("Male", "Female"))

#citizenship: 1= citizen, 2= non-citizen, set everything else to NA
merged_df$DMDCITZN[merged_df$DMDCITZN != 1 & merged_df$DMDCITZN != 2] <- NA
merged_df$DMDCITZN <- factor(merged_df$DMDCITZN, levels = c(1, 2), 
                             labels = c("Citizen", "Non-Citizen"))

#Number of children 5 years or younger in the household: 0 = 0, 1=1, 2=2, 
#3 = 3 or more, . = missing
merged_df$DMDHHSZA[merged_df$DMDHHSZA == "."] <- NA
merged_df$DMDHHSZA <- factor(merged_df$DMDHHSZA, levels = c(0, 1,2, 3), 
                             labels = c("0", "1", "2", "3 or more"))


#annual income: 15 levels - 77 = refused, 99 = don't know, . = missing
merged_df$INDHHIN2[merged_df$INDHHIN2 == "."] <- NA
merged_df$INDHHIN2[merged_df$INDHHIN2 == "77"] <- NA
merged_df$INDHHIN2[merged_df$INDHHIN2 == "99"] <- NA

#correcting the order - messed up at level 12 and on
income_levels <- c("$0 to $4,999", 
                   "$5,000 to $9,999", 
                   "$10,000 to $14,999", 
                   "$15,000 to $19,999", 
                   "$20,000 to $24,999", 
                   "$25,000 to $34,999", 
                   "$35,000 to $44,999", 
                   "$45,000 to $54,999", 
                   "$55,000 to $64,999", 
                   "$65,000 to $74,999", 
                   "$75,000 to $99,999", 
                   "$100,000 and over")

#up to level 12 is correct 
#12 and 13 are already accounted for in earlier levels so drop these
merged_df$INDHHIN2 <- factor(merged_df$INDHHIN2, levels = c(1:10, 14, 15), 
                             labels = income_levels, ordered = TRUE)


#make aure their structures are what we expect
str(merged_df$RIAGENDR)
```

How to make factors: <https://www.geeksforgeeks.org/r-language/r-factors/>.

c. The Tympanometric width measure is looks approximately like a Poisson distribution. Fit four Poisson regression models predicting a respondent’s Tympanometric width in each ear. Each model is defined below, for a specific ear and a specific set of covariates.

```{r}
#| echo: true
#install.packages("pscl")
library(pscl)
```

I used this site <https://stats.stackexchange.com/questions/8511/how-to-calculate-pseudo-r2-from-rs-logistic-regression> to find how to get the psudo $R^2$. It was used for the logistic but I looked and found that it is fine to use this in the case of poisson regression as well. Now that we have this library added we can get all the values we want for the tables.


```{r}
#| echo: true
#first model: 1R - Right ear: gender
#Tympanometric width, right ear: AUXTWIDR
poisson_model_1 <- glm(AUXTWIDR ~ RIAGENDR, merged_df, 
                        family = poisson(link = "log"))
summary(poisson_model_1)
```

```{r}
#| echo: true
#doesn't show psudo r^2 use pR2 from pscl package
print(pR2(poisson_model_1))
#need to convert coefficients to exponents to mitigate log
print(exp(coef(poisson_model_1)))
```


```{r}
#| echo: true
#second model: 2R - Right ear: gender, citizenship status (as categorical), 
#number of children (as continuous), annual household income (as continuous)
poisson_model_2 <- glm(AUXTWIDR ~ RIAGENDR + DMDCITZN + DMDHHSZA + INDHHIN2, 
                        merged_df, family = poisson(link = "log"))
summary(poisson_model_2)
```

```{r}
#| echo: true
pR2(poisson_model_2)
exp(coef(poisson_model_2))
```


```{r}
#| echo: true
#third model: 1L - Left ear: gender
#Tympanometric width, left ear : AUXTWIDL
poisson_model_3 <- glm(AUXTWIDL ~ RIAGENDR, merged_df, 
                        family = poisson(link = "log"))
summary(poisson_model_3)
```

```{r}
#| echo: true
pR2(poisson_model_3)
exp(coef(poisson_model_3))
```


```{r}
#| echo: true
#fourth model: 2L - Left ear: gender, citizenship status (as categorical), 
#number of children (as continuous), annual household income (as continuous)
poisson_model_4 <- glm(AUXTWIDL ~ RIAGENDR + DMDCITZN + DMDHHSZA + INDHHIN2, 
                        merged_df, family = poisson(link = "log"))
summary(poisson_model_4)
```

```{r}
#| echo: true
pR2(poisson_model_4)
exp(coef(poisson_model_4))
```

<https://www.dataquest.io/blog/tutorial-poisson-regression-in-r/>

Produce a table presenting the estimated incidence risk ratios for the coefficients in each model, along with the sample size for the model, the pseudo-$R^2$, and AIC values.

```{r}
#| echo: true
model_summary <- data.frame(
  Model = c("1R","2R","1L","2L"),
  Sample_Size = c(4149, 3705, 4103, 3665),
  Pseudo_R2 = c(7.774521e-05, 7.580697e-03, 1.481932e-04, 5.274216e-03),
  AIC = c(96618, 84650, 98685, 86397)
)

kable(model_summary, caption = "Model summary statistics", digits = 6)

IRR_table <- data.frame(
  Term = c("Intercept", "Gender - Female", "Citizenship - Non-citizen",
           "DMDHHSZA1", "DMDHHSZA2", "DMDHHSZA3 or more",
           "INDHHIN2.L", "INDHHIN2.Q", "INDHHIN2.C", "INDHHIN2^4",
           "INDHHIN2^5", "INDHHIN2^6", "INDHHIN2^7", "INDHHIN2^8",
           "INDHHIN2^9", "INDHHIN2^10", "INDHHIN2^11"),
   `1R` = c(84.317, 1.009, NA, NA, NA, NA,
           NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA),
  `2R` = c(83.557, 1.015, 1.044, 0.997, 0.970, 1.056,
           0.954, 0.922, 0.999, 1.022, 0.996, 1.054, 0.996, 1.030, 1.035, 0.979, 1.006),
  `1L` = c(84.758, 1.013, NA, NA, NA, NA,
           NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA),
  `2L` = c(85.121, 1.019, 1.018, 0.989, 0.969, 0.922,
           0.931, 0.984, 1.024, 0.977, 0.992, 1.062, 0.977, 0.994, 1.007, 0.970, 0.950),
  check.names = FALSE
)
kable(IRR_table, caption = "Estimated incidence risk ratios (IRR) for each model rounded to 3 decimals", digits = 3)

```
I tried to make Latex tables with latex code bc I thought quarto docs could handle it but I guess that's only for PDFs :(



d. From model 2L, provide evidence whether there is a difference between males and females in terms of their incidence risk ratio. Test whether the predicted value of Tympanometric width measure of the left ear differs between men and women. Include the results of the each test and their interpretation.

```{r}
#| echo: true
summary(poisson_model_4)
```
According to model 2L, being female is associated with a significantly higher predicted Tympanometric width in the left ear. The estimated incidence risk ratio is 1.019, meaning females are predicted to have a 1.9 percent higher value than males, holding all of the other co-variates constant. This difference is statistically significant (p < 0.001).


## Problem 2

Use the “sakila” database discussed in class. For each of the following questions, solve them in two ways: First, use SQL query or queries to extract the appropriate table(s), then use regular R operations on those data frames to answer the question. Second, use a single SQL query to answer the question. Compare each approach using microbenchmark.

```{r}
#read in data and add the needed libraries:
#install.packages("RSQLite")
library(RSQLite)
#install.packages("DBI")
library(DBI)
sakila <-  dbConnect(RSQLite::SQLite(), "sakila_master.db")
library(microbenchmark)

```

SQL version:

a. For each store, how many customers does that store have, and what percentage of customers of that store are active in the system?

```{r}
dbListTables(sakila)

#set up fucntion so don't have to write get query each time
gg <- function(query) {
  dbGetQuery(sakila, query)
}
```

```{r}
#variables we want
# #customers at each store
# #active customers
#looks like customer table has all info we need
#variable names: store_id (num), customer_id (num), active (char)
sql_a <- gg("SELECT store_id, 
                    customer_id, 
                    active
            FROM customer")

stores <- unique(sql_a$store_id) #only two unique stores?

#convert active to numeric
sql_a$active <- as.numeric(sql_a$active)

#looks like 14 are inactive of 599 total not per store (just from eye balling)
#go through stores and get counts
table(sql_a$store_id)
table(sql_a$active)

#15 total inactive but need the breakdown for the two stores
store1_total <- 0
store1_active <- 0
store2_total <- 0
store2_active <- 0

#go through the 599 rows
for (i in 1:nrow(sql_a)) {
  if (sql_a$store_id[i] == 1) {
    store1_total <- store1_total + 1
    if (sql_a$active[i] == 1) {
      store1_active <- store1_active + 1
    }
  }
  
  if (sql_a$store_id[i] == 2) {
    store2_total <- store2_total + 1
    if (sql_a$active[i] == 1) {
      store2_active <- store2_active + 1
    }
  }
}

store1_percent <- 100 * (store1_active / store1_total)
store2_percent <- 100 * (store2_active / store2_total)


cat("Percent active at store 1:", store1_percent, "%", "\n")
cat("Percent active at store 2:", store2_percent, "%", "\n")

```



b. Generate a table identifying the names and country of each staff member.

```{r}
#staff is not directly connected to country but address id gets us to city
#city table gets us to country
sql_b_staff <- gg("SELECT staff_id,
                          first_name,
                          last_name,
                          address_id
                   FROM staff")

sql_b_address <- gg("SELECT address_id,
                            city_id
                     FROM address")
sql_b_city <- gg("SELECT city_id, 
                         country_id 
                  FROM city")
  
sql_b_country <- gg("SELECT country_id, 
                            country 
                     FROM country")

check <- gg("SELECT first_name, last_name FROM staff")
check
#ONLY TWO STAFF
#now hoave to merge everything like we did in problem 1
#merge staff and address on common variable
staff_address <- merge(sql_b_staff, sql_b_address, by = "address_id")

#merge city and address on common
address_city <- merge(staff_address, sql_b_city, by = "city_id")

#merge city and country with common
city_country <- merge(staff_city, sql_b_country, by = "country_id")

#get the names
city_country$staff_name <- paste(city_country$first_name, city_country$last_name)

#only want staff ID, their name and their country
final_df <- city_country[, c("staff_id", "staff_name", "country")]

final_df
```



c. Identify the name(s) of the film(s) which was/were rented for the highest dollar value. (Assume all costs are in USD regardless of country.) (Hint: You can merge a table more than once.)

```{r}
sql_c <- gg("SELECT film_id,
                         title,
                         rental_rate 
                  FROM film")
max_rental_rate <- max(sql_c$rental_rate)
top_rent_film <- sql_c[sql_c$rental_rate == max_rental_rate, ]

top_rent_film
```


Single SQL querey:

a.

```{r}
single_a <- gg("SELECT 
               ")

```


b.

```{r}
single_b <- gg("SELECT 
               ")

```


c.

```{r}
single_c <- gg("SELECT
               ")

```



Comparisons:

```{r}
#microbenchmark for each of the different versions and questions
#part a
microbenchmark(sql_a, single_a)

```



```{r}
#part b
microbenchmark(sql_b, single_b)

```



```{r}
#part c
microbenchmark(sql_c, single_c)

```




## Problem 3

Download the ``Australia - 500 Records'' data. Use it to answer the following questions.

```{r}
#| echo: true
aus_df <- read.csv("au-500.csv")
```

a. What percentage of the websites are .com’s (as opposed to .net, .com.au, etc)?

```{r}
#| echo: true
#grep("\\.com\\.au", aus_df$web)
table(grepl("\\.com\\.au", aus_df$web))
```
Zero percent of the websites in the data set end in .com. All of the websites end in .com.au, which can be seen just taking a look at the data set but this gives us concrete proof that this is the case and our eyes aren't tricking us.

b. What is the most common domain name amongst the email addresses?

```{r}
#| echo: true
domains <- sapply(strsplit(aus_df$email, "@"), "[[", 2)
table(domains)
```

The most common domain name in the email addresses is Hotmail with 114 occurrences, followed by Gmail with 102 and Yahoo with 84. The code I used for this was taken from <https://stackoverflow.com/questions/31235165/sapply-with-strsplit-in-r> where they are keeping the part after a certain symbol.

c. What proportion of company names contain a non-alphabetic character, excluding commas and whitespace. (E.g. “Jane Doe, LLC” would not contain an eligible non-alphabetic character; “Plumber 247” would.) What about if you also exclude ampersands (“&”)?

```{r}
#| echo: true
clean_company_name <- gsub(" ", "", aus_df$company_name, fixed = TRUE)
no_comma <- gsub(",", "", clean_company_name, fixed = TRUE)
table(grepl("[^A-Za-z]", no_comma))
(45 / 500) * 100
```

Excluding commas and whitespace, we see that $9%$ of the company names have a non-alphabetic character.

```{r}
#| echo: true
no_ampersand <- gsub("&", "", no_comma, fixed = TRUE)
table(grepl("[^A-Za-z]", no_ampersand))
(4 / 500) * 100
```

Adding in the character ``&'' we then see the proportion drops down to $0.8%$, which is really small. 


d. Make all phone numbers written like cell phones. Show it works by printing the first 10 phone numbers of each column.

```{r}
#| echo: true
#my idea is to strip the landline one of the - then place them 
#such that they match the pattern of the cell phoen
landline <- gsub("-", "", aus_df$phone1, fixed = TRUE)
aus_df$phone1 <- gsub("(\\d{4})(\\d{3})(\\d{3})$","\\1-\\2-\\3", landline)
head(aus_df$phone1, 10)
head(aus_df$phone2, 10)

```

The site I used to come up with how to place the hypens in the needed spots was here: <https://stackoverflow.com/questions/41655171/what-is-an-elegant-way-to-add-dashes-in-number-strings>. Now the landline numbers match that of the cellphone. 

e. Produce a histogram of the log of the apartment numbers for all addresses. (You may assume any number at the end of the an address is an apartment number.)

```{r}
#| echo: true
#get the number after the '#' in the addresses
aus_df <- aus_df[grepl("#", aus_df$address), ]
apt_split <- apt_split <- strsplit(aus_df$address, "#")
apt_num <- sapply(apt_split,  "[[", 2)
apt_num <- as.numeric(apt_num)
head(apt_num)
hist(log(apt_num))
```
The numbers seen from head are the first apartment numbers that show up in the CSV. I used the same apply function as part b, which comes from that same site from earlier. 

f. Do you think the apartment numbers would pass as real data? Based on Benford's law

Benford's law is an observation that in many real-life sets of numerical data, the leading digit is likely to be small, meaning our histogram should show lots of values that fall into the smaller number categories compared to the larger values. From our histogram in the last part of this problem, we can see this is not the case. Our histogram of apartment numbers we see that log 0 to 1 has one of the lowest counts with a frequency of 5 then it goes up and down as we go further up on the x-axis, with no set pattern. Therefore, the apartment numbers do not pass as real data, which matches the fact that this data was made up.








